# EthioMed: AI-Enriched Medical Data Warehouse

**Author:** Maireg  
**Role:** Data Engineer at Kara Solutions  
**Status:** Final Submission (Completed Task 1â€“5)

---

## ğŸ“Œ Project Overview
EthioMed is a production-ready data engineering pipeline that transforms unstructured data from Ethiopian medical Telegram channels into a structured, AI-enriched analytical warehouse. 

By integrating **Computer Vision (YOLOv8)**, **dbt for dimensional modeling**, and **Dagster for orchestration**, the platform provides actionable insights into medical product trends, promotional strategies, and channel activity across Ethiopia.

---

## ğŸ—ï¸ Technical Architecture
The project follows a modern **ELT (Extract, Load, Transform)** architecture:

*   **Ingestion (Extract):** Telethon-based scrapers extract raw messages and images from channels like `@CheMed123` and `@tikvahpharma`.
*   **Enrichment (AI/CV):** **YOLOv8** processes images to categorize content into *Product Display*, *Promotional*, or *Lifestyle*.
*   **Storage (Load):** Raw data (JSON) and YOLO results (CSV) are loaded into a **PostgreSQL** Data Warehouse.
*   **Transformation (Transform):** **dbt** models raw data into a cleaned **Star Schema** with robust data quality tests.
*   **Serving (API):** **FastAPI** serves analytical endpoints for business reporting.
*   **Orchestration:** **Dagster** manages the end-to-end lineage, scheduling, and error handling.

---

## ğŸ“‚ Project Structure
```text
Telegram-Medical-Pipeline/
â”œâ”€â”€ api/                        # FastAPI Application
â”‚   â”œâ”€â”€ main.py                 # API Endpoints
â”‚   â”œâ”€â”€ database.py             # SQLAlchemy Connection
â”‚   â””â”€â”€ schemas.py              # Pydantic Models
â”œâ”€â”€ data/                       # Local Data Lake (GitIgnored)
â”‚   â”œâ”€â”€ raw/images/             # Scraped Media
â”‚   â””â”€â”€ yolo_results.csv        # Detection Metadata
â”œâ”€â”€ medical_warehouse/          # dbt Project
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/            # Data Cleaning & Standardizing
â”‚   â”‚   â””â”€â”€ marts/              # Fact & Dimension Tables
â”œâ”€â”€ scripts/                    # Maintenance & Loading Scripts
â”‚   â””â”€â”€ load_yolo_to_db.py      # AI Metadata Database Loader
â”œâ”€â”€ src/                        # Data Acquisition & Enrichment
â”‚   â”œâ”€â”€ scraper.py              # Telegram Extraction
â”‚   â””â”€â”€ yolo_detect.py          # YOLOv8 Object Detection
â”œâ”€â”€ pipeline.py                 # Dagster Orchestration & Scheduling
â”œâ”€â”€ docker-compose.yml          # PostgreSQL Orchestration
â””â”€â”€ requirements.txt            # Project Dependencies
âš™ï¸ Running the Pipeline
1. Start Database
code
Bash
docker compose up -d

2. Automated Orchestration (Dagster)
The entire pipeline is orchestrated via Dagster.
code
Bash
dagster dev -f pipeline.py
Access the UI at http://localhost:3000.
Hardening: The pipeline includes an automated Daily Schedule (midnight) and integrated dbt tests to ensure data quality.

3. Serve the API
Expose the data warehouse insights:
code
Bash
uvicorn api.main:app --reload
Interactive Documentation: http://127.0.0.1:8000/docs

ğŸ“Š AI Enrichment & API Analytics
The pipeline utilizes YOLOv8 to analyze image content, providing the following insights via the API:
Lifestyle Detection: Identifies brand-building lifestyle posts.
Product Display: Tracks how often raw products are showcased.
Promotional: Identifies high-value sales content featuring people and products.
Analytical Endpoints:
GET /api/reports/top-products
GET /api/reports/visual-content
GET /api/channels/{name}/activity

ğŸ›¡ï¸ Production Hardening
To move beyond a prototype, the following features were implemented:
Automated Scheduling: Configured Dagster schedules for daily data refreshes.
Integrated Testing: Automated dbt test execution within the pipeline to prevent data regression.
CI/CD Foundation: Organized project structure for automated deployment.
Author Maireg
January 20, 2026